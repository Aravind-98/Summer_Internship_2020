# Literature Review of **Index Tracking Based on Deep Neural Network-Ouyang et al**

*Note: The data used here was that of the Hang-Seng Index(HSI)*

* [2,3,4,5,6,7] analytical framework
* [8] uses metaheuristic algorithms like Genetic Algorithm for optimization while stock selection is done with Fundamental Analysis
* [10:15] use NNs and SVMs to achieve better performance
* [18]-Heaton's paper(The primary basis of this paper). They develop a deep portfolio, with the exception of not determining the stock weights, which the authors of this paper account for.

## Brief Overview of the Methodology
The problem is broken down into 2 stages: stock selection and weight allocation.
1) The stock selection problem is addressed by using what's known as a **Deep AutoEncoder**.
In a sense it can be viewed as PCA's non-linear counterpart. The basic idea is straightforward as it is.
The input is X=[x_1,x_2,...,x_n] where each **x_i is the 'Adj closing price' time series of the ith stock**. This input is compressed to lower-dimensional form by a hidden layer of neurons in the middle and then expanded again back to original dimensions. There is some loss of information here, due to compression process. We train the network in such a manner that this loss of information mainly accounts for the statistical noise.
Due to non-linearity, this is generally more powerful than the PCA. 

*It is an unsupervised learning problem where the input is trained on itself (ie X_hat=F(X)).*

The low-dimensional encoded data Z generated by the encoder process equation can be
considered as the market portfolio composed of n stocks and contains the nonlinear interactive effect
between different stocks. The output data X (i.e., the data after decoding) generated by equation
reflects the nonlinear impact of market portfolio on different stocks.

Further some similarities and distinctions are drawn between CAPM and this process.
I intend to use a sequence to sequence LSTM autoencoder which I assume would perform better given that it accounts for time-dependencies across our time series data.
In my case, I plan to use a data vector as input of each time-step, this n-d vector has #number_of_stocks(n) number of elements in it(46) in our case. 
![seq2seq model](https://miro.medium.com/max/5028/1*tY4F3BPq4ctTMelMEnLZvw.png)

Once we have the denoised representation of out data, the author then focuses on finding the similarity between each stock and it's denoised representation. He computes this by simply taking **d_i=(||x_i - x_i_hat ||)^(2)**
He then takes the 'h' most similar stocks and 'l' least similar stocks such that h<l (this can be obtained by the sorting the distance measure).

This finishes the stock selection process.

**Note: This is the portion of the paper that I am not satisfied with! I have observed and compared the results of my work as well as that of the author, and I don't seem to obtain a good result for denoising. I wish to discuss this further with you, Mam.**


2) The author trains a standard Deep Neural Network and devises a method to compute the weights used by taking non-linear activations like 'ReLU' for convenience to compute the weights of the corresponding stocks.
I have tried the same using a RNN/LSTM based approach to account for time-dependence. However as a drawback, owing to the complications of my model, I wouldn't be able to find the weights. I will be explaing the details of my model below.

___
*I will briefly **explain the RNN cell and architecture** along with its role in my ideation of the problem.*
![RNN architecture](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)

Basically, we have just one cell which takes in a input(one training example) say *x<t>*, does some non-linear transformations on it and gives a output say *y<t>* and passes a primitive form of the output say *a<t>* to be used in the next time step. And so on, the cell recurs for as many time steps as specified and passing the information along time. In our case, we have T{1708:train and 733:test} timesteps with the input at each time step being a K-dimensional(corresponding to number of stocks chosen) vector of data. So, at timestep 1 we perform some computations give and try to predict the benchmark price with the K-D vector and pass along the intermediate info through time. The later timesteps get the info of the previous values used to try and predict a better value when it's upto them. 
  
![RNN Cell](https://www.researchgate.net/publication/332663947/figure/fig1/AS:751783865511938@1556250649554/Simple-RNN-cell-structure-in-hidden-layer-b.png)

LSTM and GRU architectures only vary in the basic type of cell being used. RNNs have an issue in remembering data too far down the line. LSTM/GRUs come out as good alternatives then as they have certian gates within the cells which allow them to remember and forget information whenever necessary.
![LSTM and GRU](https://miro.medium.com/max/4212/1*E18UBv1G7nAaq-soA-zBlw.png)
___

* [Advantages of LSTM autoencoders vs Normal autoencoders ](https://www.quora.com/What-are-advantages-of-LSTM-autoencoders-over-normal-autoencoders)
* [Example of rare event classification using LSTM Autoencoders](https://towardsdatascience.com/lstm-autoencoder-for-extreme-rare-event-classification-in-keras-ce209a224cfb)
```
Papers required to be researched further:
* Hinton G E, Salakhutdinov R R. (2006). Reducing the dimensionality of data with neural networks.
Science, 313(5786):504.
* Hu, C. P., Xue, H. G., & Xu, F, M. (2014). A stock index replicating model based on time weighted
svm and it's empirical analysis. Systems Engineering-Theory & Practice, 34(9), 2193-2201.
* Fernandez, A., & Gomez, S. (2007). Portfolio selection using neural networks. Computers &
Operations Research, 34(4), 1177-1191.
* Cai, Y. P., Wan, L, Fan, X. D. (2006). An empirical comparison of alternative models of index fund.
Journal of Quantitative & Technical Economics, 23(10), 130-140.
* Jian, G., Yang D. (2006). A comparative analysis of replication method of index
investment. Journal of Financial Research,51(8), 31-40.
```
